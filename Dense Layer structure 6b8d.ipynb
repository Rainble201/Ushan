{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a2e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 10:57:59.614828: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from scipy.stats import norm\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6253eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples_surface_plot = 21\n",
    "nr_samples_scatter_plot = 1000\n",
    "nr_samples_error_calculation = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e27b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters. Re-train model after any changes.\n",
    "s_min_interest = 25\n",
    "s_max_interest = 150\n",
    "t_min_interest = 0.5\n",
    "t_max_interest = 4.\n",
    "\n",
    "riskfree_rate_min = 0.1\n",
    "riskfree_rate_max = 0.3\n",
    "riskfree_rate_eval = 0.2\n",
    "\n",
    "volatility_min = 0.1\n",
    "volatility_max = 0.3\n",
    "volatility1_eval = 0.1\n",
    "volatility2_eval = 0.3\n",
    "\n",
    "correlation_min = 0.2\n",
    "correlation_max = 0.8\n",
    "correlation_eval = 0.5\n",
    "\n",
    "strike_price = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db46ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_nodes_per_layer = 90\n",
    "initial_learning_rate = 0.001\n",
    "localisation_parameter = 1/10.\n",
    "\n",
    "n_train = 10000\n",
    "nr_epochs = 601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7ef97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_state = 2\n",
    "dimension_parameter = 4\n",
    "dimension_total = 1 + dimension_state + dimension_parameter\n",
    "\n",
    "t_min = 0.\n",
    "t_max = t_max_interest\n",
    "s_max = strike_price * (1 + 3*volatility_max*t_max)\n",
    "x_max = np.log(s_max)\n",
    "x_min = 2*np.log(strike_price) - x_max\n",
    "\n",
    "normalised_max = 1\n",
    "normalised_min = -1\n",
    "\n",
    "def transform_ab_to_cd(x, a, b, c, d): \n",
    "    \"\"\"\n",
    "    Perform a linear transformation of a scalar from the souce interval\n",
    "    to the target interval.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- scalar point(s) to transform\n",
    "    a, b -- interval to transform from\n",
    "    c, d -- interval to transform to \n",
    "    \"\"\"\n",
    "    return c + (x-a) * (d-c) / (b-a)\n",
    "\n",
    "def transform_to_logprice(x): \n",
    "    \"\"\" Transform normalised variable to the log-price. \"\"\"\n",
    "    return transform_ab_to_cd(x, normalised_min, normalised_max, x_min, x_max)\n",
    "\n",
    "def transform_to_time(t): \n",
    "    \"\"\" Transform normalised variable to the time variable. \"\"\"\n",
    "    return transform_ab_to_cd(t, normalised_min, normalised_max, t_min, t_max)\n",
    "\n",
    "def normalise_logprice(x):\n",
    "    \"\"\" Transform log-price to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(x, x_min, x_max, normalised_min, normalised_max)\n",
    "\n",
    "def normalise_time(t): \n",
    "    \"\"\" Transform time to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(t, t_min, t_max, normalised_min, normalised_max)\n",
    "\n",
    "t_min_interest_normalised = normalise_time(t_min_interest)\n",
    "t_max_interest_normalised = normalise_time(t_max_interest)\n",
    "\n",
    "diff_dx = (normalised_max-normalised_min) / (x_max-x_min) \n",
    "diff_dt = (normalised_max-normalised_min) / (t_max-t_min)\n",
    "\n",
    "def transform_to_riskfree_rate(mu_1):\n",
    "    \"\"\" Transform normalised variable to the risk-free rate. \"\"\"\n",
    "    return transform_ab_to_cd(mu_1, normalised_min, normalised_max,\n",
    "                                    riskfree_rate_min, riskfree_rate_max)\n",
    "\n",
    "def transform_to_volatility(mu_2):\n",
    "    \"\"\" Transform normalised variable to the volatility. \"\"\"\n",
    "    return transform_ab_to_cd(mu_2, normalised_min, normalised_max,\n",
    "                                    volatility_min, volatility_max)\n",
    "    \n",
    "def transform_to_correlation(mu_3):\n",
    "    \"\"\" Transform normalised variable to the correlation. \"\"\"\n",
    "    return transform_ab_to_cd(mu_3, normalised_min, normalised_max,\n",
    "                                    correlation_min, correlation_max)\n",
    "\n",
    "def normalise_riskfree_rate(riskfree_rate):\n",
    "    \"\"\" Transform risk-free rate to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(riskfree_rate,\n",
    "                              riskfree_rate_min, riskfree_rate_max, \n",
    "                              normalised_min, normalised_max)\n",
    "    \n",
    "def normalise_volatility(volatility):\n",
    "    \"\"\" Transform volatility to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd( volatility, volatility_min, volatility_max,\n",
    "                                            normalised_min, normalised_max)\n",
    "    \n",
    "def normalise_correlation(correlation):\n",
    "    \"\"\" Transform correlation to its corresponding normalised variable. \"\"\"\n",
    "    return transform_ab_to_cd(correlation, correlation_min, correlation_max,\n",
    "                                            normalised_min, normalised_max)\n",
    "\n",
    "\n",
    "riskfree_rate_eval_normalised = normalise_riskfree_rate(riskfree_rate_eval)\n",
    "volatility1_eval_normalised = normalise_volatility(volatility1_eval)\n",
    "volatility2_eval_normalised = normalise_volatility(volatility2_eval)\n",
    "correlation_eval_normalised = normalise_correlation(correlation_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b47f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(keras.layers.Layer):\n",
    "    \"\"\" Define one layer of the dense network. \"\"\"\n",
    "\n",
    "    def __init__(self, units=90, input_dim=90):\n",
    "        \"\"\" Construct the layer by creating all weights and biases in keras. \"\"\"\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        # create all weights and biases\n",
    "        self.W = self.add_weight(\"W\", shape=(input_dim, self.units),\n",
    "                                 initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(\"b\", shape=(self.units,),\n",
    "                                 initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, input_combined):\n",
    "        \"\"\" Returns the result of the layer calculation.\n",
    "\n",
    "        Keyword arguments:\n",
    "        input_combined -- Dictionary containing the original input of\n",
    "        the neural network as 'original_variable' and\n",
    "        the output of the previous layer as 'previous_layer'.\n",
    "        \"\"\"\n",
    "        original_variable = input_combined['original_variable']\n",
    "        \n",
    "        # Evaluate one layer using the weights created by the constructor\n",
    "        output = tf.matmul(original_variable, self.W) + self.b\n",
    "        output = tf.keras.activations.tanh(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f50b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(inputs):\n",
    "        # Layer 0: Dense layer of 90 nodes\n",
    "    layer0 = DenseLayer(units=90, input_dim=dimension_total)\n",
    "    outputs_layer0 = layer0({'original_variable': inputs})\n",
    "\n",
    "    # Block 1: 8x Dense Layer of 90 nodes + Output of Layer 0\n",
    "    layer1_1 = DenseLayer()\n",
    "    outputs_layer1_1 = layer1_1({'original_variable': outputs_layer0})\n",
    "    layer1_2 = DenseLayer()\n",
    "    outputs_layer1_2 = layer1_2({'original_variable': outputs_layer1_1})\n",
    "    # ... similarly for layer1_3 to layer1_8\n",
    "    layer1_3 = DenseLayer()\n",
    "    outputs_layer1_3 = layer1_3({'original_variable': outputs_layer1_2})\n",
    "    layer1_4 = DenseLayer()\n",
    "    outputs_layer1_4 = layer1_4({'original_variable': outputs_layer1_3})\n",
    "    layer1_5 = DenseLayer()\n",
    "    outputs_layer1_5 = layer1_5({'original_variable': outputs_layer1_4})\n",
    "    layer1_6 = DenseLayer()\n",
    "    outputs_layer1_6 = layer1_6({'original_variable': outputs_layer1_5})\n",
    "    layer1_7 = DenseLayer()\n",
    "    outputs_layer1_7 = layer1_7({'original_variable': outputs_layer1_6})\n",
    "    layer1_8 = DenseLayer()\n",
    "    outputs_layer1_8 = layer1_8({'original_variable': outputs_layer1_7})\n",
    "    outputs_block1 = outputs_layer0 + outputs_layer1_8\n",
    "\n",
    "    # Block 2: 8x Dense Layer of 90 nodes + Output of Block 1\n",
    "    layer2_1 = DenseLayer()\n",
    "    outputs_layer2_1 = layer2_1({'original_variable': outputs_block1})\n",
    "    layer2_2 = DenseLayer()\n",
    "    outputs_layer2_2 = layer2_2({'original_variable': outputs_layer2_1})\n",
    "    # ... similarly for layer2_3 to layer2_8\n",
    "    layer2_3 = DenseLayer()\n",
    "    outputs_layer2_3 = layer2_3({'original_variable': outputs_layer2_2})\n",
    "    layer2_4 = DenseLayer()\n",
    "    outputs_layer2_4 = layer2_4({'original_variable': outputs_layer2_3})\n",
    "    layer2_5 = DenseLayer()\n",
    "    outputs_layer2_5 = layer2_5({'original_variable': outputs_layer2_4})\n",
    "    layer2_6 = DenseLayer()\n",
    "    outputs_layer2_6 = layer2_6({'original_variable': outputs_layer2_5})\n",
    "    layer2_7 = DenseLayer()\n",
    "    outputs_layer2_7 = layer2_7({'original_variable': outputs_layer2_6})\n",
    "    layer2_8 = DenseLayer()\n",
    "    outputs_layer2_8 = layer2_8({'original_variable': outputs_layer2_7})\n",
    "    outputs_block2 = outputs_block1 + outputs_layer2_8\n",
    "\n",
    "    # Block 3: 8x Dense Layer of 90 nodes + Output of Block 2\n",
    "    layer3_1 = DenseLayer()\n",
    "    outputs_layer3_1 = layer3_1({'original_variable': outputs_block2})\n",
    "    layer3_2 = DenseLayer()\n",
    "    outputs_layer3_2 = layer3_2({'original_variable': outputs_layer3_1})\n",
    "    # ... similarly for layer3_3 to layer3_8\n",
    "    layer3_3 = DenseLayer()\n",
    "    outputs_layer3_3 = layer3_3({'original_variable': outputs_layer3_2})\n",
    "    layer3_4 = DenseLayer()\n",
    "    outputs_layer3_4 = layer3_4({'original_variable': outputs_layer3_3})\n",
    "    layer3_5 = DenseLayer()\n",
    "    outputs_layer3_5 = layer3_5({'original_variable': outputs_layer3_4})\n",
    "    layer3_6 = DenseLayer()\n",
    "    outputs_layer3_6 = layer3_6({'original_variable': outputs_layer3_5})\n",
    "    layer3_7 = DenseLayer()\n",
    "    outputs_layer3_7 = layer3_7({'original_variable': outputs_layer3_6})\n",
    "    layer3_8 = DenseLayer()\n",
    "    outputs_layer3_8 = layer3_8({'original_variable': outputs_layer3_7})\n",
    "    outputs_block3 = outputs_block2 + outputs_layer3_8\n",
    "    \n",
    "    # Block 4: 8x Dense Layer of 90 nodes + Output of Block 3\n",
    "    layer4_1 = DenseLayer()\n",
    "    outputs_layer4_1 = layer4_1({'original_variable': outputs_block3})\n",
    "    layer4_2 = DenseLayer()\n",
    "    outputs_layer4_2 = layer4_2({'original_variable': outputs_layer4_1})\n",
    "    layer4_3 = DenseLayer()\n",
    "    outputs_layer4_3 = layer4_3({'original_variable': outputs_layer4_2})\n",
    "    layer4_4 = DenseLayer()\n",
    "    outputs_layer4_4 = layer4_4({'original_variable': outputs_layer4_3})\n",
    "    layer4_5 = DenseLayer()\n",
    "    outputs_layer4_5 = layer4_5({'original_variable': outputs_layer4_4})\n",
    "    layer4_6 = DenseLayer()\n",
    "    outputs_layer4_6 = layer4_6({'original_variable': outputs_layer4_5})\n",
    "    layer4_7 = DenseLayer()\n",
    "    outputs_layer4_7 = layer4_7({'original_variable': outputs_layer4_6})\n",
    "    layer4_8 = DenseLayer()\n",
    "    outputs_layer4_8 = layer4_8({'original_variable': outputs_layer4_7})\n",
    "    outputs_block4 = outputs_block3 + outputs_layer4_8\n",
    "    \n",
    "        # Block 5: 8x Dense Layer of 90 nodes + Output of Block 4\n",
    "    layer5_1 = DenseLayer()\n",
    "    outputs_layer5_1 = layer5_1({'original_variable': outputs_block4})\n",
    "    layer5_2 = DenseLayer()\n",
    "    outputs_layer5_2 = layer5_2({'original_variable': outputs_layer5_1})\n",
    "    layer5_3 = DenseLayer()\n",
    "    outputs_layer5_3 = layer5_3({'original_variable': outputs_layer5_2})\n",
    "    layer5_4 = DenseLayer()\n",
    "    outputs_layer5_4 = layer5_4({'original_variable': outputs_layer5_3})\n",
    "    layer5_5 = DenseLayer()\n",
    "    outputs_layer5_5 = layer5_5({'original_variable': outputs_layer5_4})\n",
    "    layer5_6 = DenseLayer()\n",
    "    outputs_layer5_6 = layer5_6({'original_variable': outputs_layer5_5})\n",
    "    layer5_7 = DenseLayer()\n",
    "    outputs_layer5_7 = layer5_7({'original_variable': outputs_layer5_6})\n",
    "    layer5_8 = DenseLayer()\n",
    "    outputs_layer5_8 = layer5_8({'original_variable': outputs_layer5_7})\n",
    "    outputs_block5 = outputs_block4 + outputs_layer5_8\n",
    "\n",
    "    # Block 6: 8x Dense Layer of 90 nodes + Output of Block 5\n",
    "    layer6_1 = DenseLayer()\n",
    "    outputs_layer6_1 = layer6_1({'original_variable': outputs_block5})\n",
    "    layer6_2 = DenseLayer()\n",
    "    outputs_layer6_2 = layer6_2({'original_variable': outputs_layer6_1})\n",
    "    layer6_3 = DenseLayer()\n",
    "    outputs_layer6_3 = layer6_3({'original_variable': outputs_layer6_2})\n",
    "    layer6_4 = DenseLayer()\n",
    "    outputs_layer6_4 = layer6_4({'original_variable': outputs_layer6_3})\n",
    "    layer6_5 = DenseLayer()\n",
    "    outputs_layer6_5 = layer6_5({'original_variable': outputs_layer6_4})\n",
    "    layer6_6 = DenseLayer()\n",
    "    outputs_layer6_6 = layer6_6({'original_variable': outputs_layer6_5})\n",
    "    layer6_7 = DenseLayer()\n",
    "    outputs_layer6_7 = layer6_7({'original_variable': outputs_layer6_6})\n",
    "    layer6_8 = DenseLayer()\n",
    "    outputs_layer6_8 = layer6_8({'original_variable': outputs_layer6_7})\n",
    "    outputs_block6 = outputs_block5 + outputs_layer6_8\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Output Layer: Dense layer of 1 node\n",
    "    last_layer = keras.layers.Dense(1)\n",
    "    outputs_dnn = last_layer(outputs_block6)\n",
    "    \n",
    "    # Remaining part of the network that was provided before\n",
    "    inputs_t_normalised = inputs[:, 0:1]\n",
    "    inputs_x1_normalised = inputs[:, 1:2]\n",
    "    inputs_x2_normalised = inputs[:, 2:3]\n",
    "    inputs_p1_normalised = inputs[:, 3:4]\n",
    "    \n",
    "    inputs_t = transform_to_time(inputs_t_normalised)\n",
    "    inputs_x1 = transform_to_logprice(inputs_x1_normalised)\n",
    "    inputs_x2 = transform_to_logprice(inputs_x2_normalised)\n",
    "    inputs_s_mean = (tf.math.exp(inputs_x1) + tf.math.exp(inputs_x2))/2.\n",
    "    riskfree_rate = transform_to_riskfree_rate(inputs_p1_normalised)\n",
    "\n",
    "    localisation = tf.math.log(1+tf.math.exp(localisation_parameter * (\n",
    "            inputs_s_mean - strike_price * tf.exp( - riskfree_rate * inputs_t)\n",
    "              )))/localisation_parameter\n",
    "\n",
    "    return outputs_dnn + localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f234ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPDEGenerator(keras.utils.Sequence):\n",
    "    \"\"\" Create batches of random points for the network training. \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        \"\"\" Initialise the generator by saving the batch size. \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "      \n",
    "    def __len__(self):\n",
    "        \"\"\" Describes the number of points to create \"\"\"\n",
    "        return self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get one batch of random points in the interior of the domain to \n",
    "        train the PDE residual and with initial time to train the initial value.\n",
    "        \"\"\"\n",
    "        data_train_interior = np.random.uniform(\n",
    "            normalised_min, normalised_max, [self.batch_size, dimension_total]) \n",
    "\n",
    "        t_train_initial = normalised_min * np.ones((self.batch_size, 1))\n",
    "        s_and_p_train_initial = np.random.uniform(\n",
    "            normalised_min, normalised_max,\n",
    "            [self.batch_size, dimension_state + dimension_parameter])\n",
    "        \n",
    "        data_train_initial = np.concatenate(\n",
    "            (t_train_initial, s_and_p_train_initial), axis=1)\n",
    "\n",
    "        return [data_train_interior, data_train_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b84c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPDEModel(keras.Model):\n",
    "    \"\"\" Create a keras model with the deep param. PDE loss function \"\"\"\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\" Create one optimisation stop based on the deep param. PDE loss function. \"\"\"\n",
    "        data_interior, data_initial = data\n",
    "\n",
    "        riskfree_rate_interior = transform_to_riskfree_rate(\n",
    "            data_interior[:, 3:4])\n",
    "        volatility1_interior = transform_to_volatility(data_interior[:, 4:5])\n",
    "        volatility2_interior = transform_to_volatility(data_interior[:, 5:6])\n",
    "        correlation_interior = transform_to_correlation(data_interior[:, 6:7])\n",
    "\n",
    "        x1_initial = transform_to_logprice(data_initial[:, 1:2])\n",
    "        x2_initial = transform_to_logprice(data_initial[:, 2:3])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_interior = self(data_interior, training=True)  # Forward pass\n",
    "            v_initial = self(data_initial, training=True)  # Forward pass bdry\n",
    "\n",
    "            gradient = K.gradients(v_interior, data_interior)[0]\n",
    "\n",
    "            v_dt = diff_dt * gradient[:, 0:1]\n",
    "            v_dx1 = diff_dx * gradient[:, 1:2]\n",
    "            v_dx2 = diff_dx * gradient[:, 2:3]\n",
    "\n",
    "            grad_v_dx1 = K.gradients(v_dx1, data_interior)[0]\n",
    "            grad_v_dx2 = K.gradients(v_dx2, data_interior)[0]\n",
    "\n",
    "            v_dx1dx1 = diff_dx * grad_v_dx1[:, 1:2]\n",
    "            v_dx2dx2 = diff_dx * grad_v_dx2[:, 2:3]\n",
    "            v_dx1dx2 = diff_dx * grad_v_dx1[:, 2:3]\n",
    "            v_dx2dx1 = diff_dx * grad_v_dx2[:, 1:2]\n",
    "\n",
    "            residual_interior = (\n",
    "                v_dt + riskfree_rate_interior * v_interior \n",
    "                - (riskfree_rate_interior - volatility1_interior**2/2) * v_dx1\n",
    "                - (riskfree_rate_interior - volatility2_interior**2/2) * v_dx2\n",
    "                - 0.5 * volatility1_interior**2 * v_dx1dx1\n",
    "                - 0.5 * volatility2_interior**2 * v_dx2dx2 \n",
    "                - 0.5 * correlation_interior \n",
    "                    * volatility1_interior * volatility2_interior * v_dx1dx2 \n",
    "                - 0.5 * correlation_interior \n",
    "                    * volatility2_interior * volatility1_interior * v_dx2dx1\n",
    "                )\n",
    "\n",
    "            s_mean_initial = 0.5 * (\n",
    "                tf.math.exp(x1_initial)+tf.math.exp(x2_initial)) \n",
    "            payoff_initial = K.maximum(s_mean_initial - strike_price, 0)\n",
    "\n",
    "            loss_interior = K.mean(K.square(residual_interior))\n",
    "            loss_initial = K.mean(K.square(v_initial - payoff_initial))\n",
    "            \n",
    "            loss = loss_initial + loss_interior\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"loss initial\": loss_initial, \n",
    "                \"loss interior\": loss_interior}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/601\n",
      " 8/10 [=======================>......] - ETA: 2s - loss: 2.8148 - loss initial: 2.2775 - loss interior: 0.5373"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "    # Create and train model from scratch. \n",
    "    inputs = keras.Input(shape=(dimension_total,))\n",
    "    outputs = create_network(inputs)\n",
    "    model = DPDEModel(inputs=inputs, outputs=outputs)\n",
    "    batch_generator = DPDEGenerator(n_train)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(initial_learning_rate))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        'loss', patience=50, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(x=batch_generator, epochs=nr_epochs, steps_per_epoch=10,\n",
    "                          callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_covariance_matrix(t, volatility1, volatility2, correlation):\n",
    "    \"\"\" Decompose covariance matrix as in Lemma 3.1 of Bayer et. al (2018). \"\"\"\n",
    "    sigma_det = (1-correlation**2) * volatility1**2 * volatility2**2\n",
    "    sigma_sum = (volatility1**2 + volatility2**2 \n",
    "                  - 2*correlation*volatility1*volatility2)\n",
    "\n",
    "    ev1 = volatility1**2 - correlation*volatility1*volatility2\n",
    "    ev2 = -(volatility2**2 - correlation*volatility1*volatility2)\n",
    "    ev_norm = np.sqrt(ev1**2 + ev2**2)\n",
    "\n",
    "    eigenvalue = volatility1**2 + volatility2**2 - 2*sigma_det/sigma_sum\n",
    "\n",
    "    v_mat = np.array([ev1, ev2]) / ev_norm\n",
    "    d = t * np.array([sigma_det/sigma_sum, eigenvalue])\n",
    "    return d, v_mat\n",
    "\n",
    "def one_dimensional_exact_solution(\n",
    "        t, s, riskfree_rate, volatility, strike_price):\n",
    "    \"\"\" Standard Black-Scholes formula \"\"\"\n",
    "\n",
    "    d1 = (1 / (volatility*np.sqrt(t))) * (\n",
    "            np.log(s/strike_price) \n",
    "            + (riskfree_rate + volatility**2/2.) * t\n",
    "        )\n",
    "    d2 = d1 - volatility*np.sqrt(t)\n",
    "    return (norm.cdf(d1) * s \n",
    "            - norm.cdf(d2) * strike_price * np.exp(-riskfree_rate*t))\n",
    "\n",
    "def exact_solution(\n",
    "    t, s1, s2, riskfree_rate, volatility1, volatility2, correlation):\n",
    "    \"\"\" Compute the option price of a European basket call option. \"\"\"\n",
    "    if t == 0:\n",
    "        return np.maximum(0.5*(s1+s2) - strike_price, 0)\n",
    "\n",
    "    d, v = decompose_covariance_matrix(\n",
    "        t, volatility1, volatility2, correlation)\n",
    "    \n",
    "    beta = [0.5 * s1 * np.exp(-0.5*t*volatility1**2),\n",
    "            0.5 * s2 * np.exp(-0.5*t*volatility2**2)]\n",
    "    integration_points, integration_weights = hermgauss(33)\n",
    "\n",
    "    # Transform points and weights\n",
    "    integration_points = np.sqrt(2*d[1]) * integration_points.reshape(-1, 1)\n",
    "    integration_weights = integration_weights.reshape(1, -1) / np.sqrt(np.pi)\n",
    "\n",
    "    h_z = (beta[0] * np.exp(v[0]*integration_points)\n",
    "           + beta[1] * np.exp(v[1]*integration_points))\n",
    "\n",
    "    evaluation_at_integration_points = one_dimensional_exact_solution(\n",
    "        t=1, s=h_z * np.exp(0.5*d[0]), \n",
    "        strike_price=np.exp(-riskfree_rate * t) * strike_price, \n",
    "        volatility=np.sqrt(d[0]), riskfree_rate=0.\n",
    "        )\n",
    "    \n",
    "    solution = np.matmul(integration_weights, evaluation_at_integration_points)\n",
    "    \n",
    "    return solution[0, 0]\n",
    "\n",
    "test_solution = exact_solution(t=4., s1=100., s2=100., riskfree_rate=0.2, \n",
    "               volatility1=0.1, volatility2=0.3, correlation=0.5)\n",
    "assert(np.abs(test_solution - 55.096796282039364) < 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localisation(t, s1, s2, riskfree_rate=riskfree_rate_eval):\n",
    "    \"\"\" Return the value of the localisation used in the network. \"\"\"\n",
    "    return 1/localisation_parameter * np.log(1 +\n",
    "                    np.exp(localisation_parameter * (\n",
    "                        0.5*(s1+s2) - np.exp(-riskfree_rate*t)*strike_price))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_points_of_interest(nr_samples, \n",
    "                    t_min_interest=t_min_interest,\n",
    "                    t_max_interest=t_max_interest,\n",
    "                    s_min_interest=s_min_interest,\n",
    "                    s_max_interest=s_max_interest,\n",
    "                    parameter_min_interest_normalised=normalised_min,\n",
    "                    parameter_max_interest_normalised=normalised_max):\n",
    "    \"\"\" Get a number of random points within the defined domain of interest. \"\"\"\n",
    "    t_sample = np.random.uniform(t_min_interest, t_max_interest, \n",
    "                                 [nr_samples, 1])\n",
    "    t_sample_normalised = normalise_time(t_sample)\n",
    "\n",
    "    s_sample = np.random.uniform(\n",
    "        s_min_interest, s_max_interest, [nr_samples, dimension_state])\n",
    "    s1_sample = s_sample[:, 0:1]\n",
    "    s2_sample = s_sample[:, 1:2]\n",
    "    x_sample_normalised = normalise_logprice(np.log(s_sample))\n",
    "\n",
    "    parameter_sample_normalised = np.random.uniform(\n",
    "        normalised_min, normalised_max, [nr_samples, dimension_parameter])\n",
    "    data_normalised = np.concatenate(\n",
    "        (t_sample_normalised, x_sample_normalised, parameter_sample_normalised),\n",
    "        axis=1\n",
    "        )\n",
    "\n",
    "    riskfree_rate_sample = transform_to_riskfree_rate(\n",
    "        parameter_sample_normalised[:, 0])\n",
    "    volatility1_sample = transform_to_volatility(\n",
    "        parameter_sample_normalised[:, 1])\n",
    "    volatility2_sample = transform_to_volatility(\n",
    "        parameter_sample_normalised[:, 2])\n",
    "    correlation_sample = transform_to_correlation(\n",
    "        parameter_sample_normalised[:, 3])\n",
    "    \n",
    "    return data_normalised, t_sample.reshape(-1), s1_sample.reshape(-1), \\\n",
    "            s2_sample.reshape(-1), riskfree_rate_sample, volatility1_sample, \\\n",
    "            volatility2_sample, correlation_sample\n",
    "\n",
    "\n",
    "def get_points_for_plot_at_fixed_time(t_fixed=t_max,\n",
    "                s_min_interest=s_min_interest, s_max_interest=s_max_interest,\n",
    "                riskfree_rate_fixed=riskfree_rate_eval,\n",
    "                volatility1_fixed=volatility1_eval,\n",
    "                volatility2_fixed=volatility2_eval,\n",
    "                correlation_fixed=correlation_eval,\n",
    "                n_plot=nr_samples_surface_plot):\n",
    "    \"\"\" Get the spacial and normalised values for surface plots \n",
    "    at fixed time and parameter, varying both asset prices. \n",
    "    \"\"\"\n",
    "    s1_plot = np.linspace(s_min_interest, s_max_interest, n_plot).reshape(-1,1)\n",
    "    s2_plot = np.linspace(s_min_interest, s_max_interest, n_plot).reshape(-1,1)\n",
    "    [s1_plot_mesh, s2_plot_mesh] = np.meshgrid(s1_plot, s2_plot, indexing='ij')\n",
    "\n",
    "    x1_plot_mesh_normalised = normalise_logprice(\n",
    "        np.log(s1_plot_mesh)).reshape(-1,1)\n",
    "\n",
    "    x2_plot_mesh_normalised = normalise_logprice(\n",
    "        np.log(s2_plot_mesh)).reshape(-1,1)\n",
    "\n",
    "    t_mesh = t_fixed  * np.ones((n_plot**2, 1))\n",
    "    t_mesh_normalised = normalise_time(t_mesh)\n",
    "\n",
    "    parameter1_mesh_normalised = (normalise_riskfree_rate(riskfree_rate_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "    parameter2_mesh_normalised = (normalise_volatility(volatility1_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "    parameter3_mesh_normalised = (normalise_volatility(volatility2_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "    parameter4_mesh_normalised = (normalise_correlation(correlation_fixed) \n",
    "                                                      * np.ones((n_plot**2, 1)))\n",
    "\n",
    "    x_plot_normalised = np.concatenate((t_mesh_normalised,\n",
    "                                        x1_plot_mesh_normalised,\n",
    "                                        x2_plot_mesh_normalised,\n",
    "                                        parameter1_mesh_normalised, \n",
    "                                        parameter2_mesh_normalised,\n",
    "                                        parameter3_mesh_normalised, \n",
    "                                        parameter4_mesh_normalised), axis=1)\n",
    "\n",
    "    \n",
    "    return s1_plot_mesh, s2_plot_mesh, x_plot_normalised\n",
    "\n",
    "\n",
    "s1_plot_mesh, s2_plot_mesh, x_plot_normalised = \\\n",
    "    get_points_for_plot_at_fixed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPDE_solution = model.predict(x_plot_normalised).reshape(\n",
    "    nr_samples_surface_plot, nr_samples_surface_plot)\n",
    "\n",
    "exact_solution_evaluated = [exact_solution(t=t_max, s1=s1[0], s2=s2[0], \n",
    "                                riskfree_rate=riskfree_rate_eval, \n",
    "                                volatility1=volatility1_eval, \n",
    "                                volatility2=volatility2_eval,\n",
    "                                correlation=correlation_eval)\n",
    "                  for s1, s2 in zip(\n",
    "                      s1_plot_mesh.reshape(-1, 1), s2_plot_mesh.reshape(-1, 1))\n",
    "                  \n",
    "                  ]\n",
    "exact_solution_evaluated = np.array(exact_solution_evaluated)\n",
    "exact_solution_evaluated = exact_solution_evaluated.reshape(\n",
    "    nr_samples_surface_plot, nr_samples_surface_plot)\n",
    "\n",
    "localisation_plot = localisation(4., s1_plot_mesh, s2_plot_mesh, riskfree_rate_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294159f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, DPDE_solution, cmap='viridis')\n",
    "ax.set_title('DPDE Solution')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a760c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "error = np.abs(DPDE_solution - exact_solution_evaluated)\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, error, cmap='viridis')\n",
    "ax.set_title('Error')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429df0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "dnn_part = DPDE_solution - localisation_plot\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, dnn_part, cmap='viridis')\n",
    "ax.set_title('DNN part')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, localisation_plot, cmap='viridis')\n",
    "ax.set_title('Localisation')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb7999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(s1_plot_mesh, s2_plot_mesh, exact_solution_evaluated, cmap='viridis')\n",
    "ax.set_title('Exact solution')\n",
    "ax.set_xlabel('$s_1$')\n",
    "ax.set_ylabel('$s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ce05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples, t_samples, s1_samples, s2_samples, riskfree_rate_samples, \\\n",
    "  volatility1_samples, volatility2_samples, correlation_samples = \\\n",
    "              get_random_points_of_interest(nr_samples_scatter_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predict {} values and measure the time:'.format(nr_samples_scatter_plot))\n",
    "%time DPDE_solution = model.predict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_solution_evaluated = [exact_solution(t=t, s1=s1, s2=s2,\n",
    "                                  riskfree_rate=riskfree_rate,\n",
    "                                  volatility1=volatility1,\n",
    "                                  volatility2=volatility2,\n",
    "                                  correlation=correlation\n",
    "                                  ) \n",
    "                  for t, s1, s2, riskfree_rate, volatility1, volatility2,\n",
    "                      correlation \n",
    "                  in zip(t_samples, s1_samples, s2_samples, riskfree_rate_samples, \n",
    "                          volatility1_samples, volatility2_samples,\n",
    "                          correlation_samples)]\n",
    "\n",
    "exact_solution_evaluated = np.array(exact_solution_evaluated).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(exact_solution_evaluated, DPDE_solution)\n",
    "plt.xlabel('Exact Solution')\n",
    "plt.ylabel('DPDE Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(exact_solution_evaluated, \n",
    "            np.abs(exact_solution_evaluated - DPDE_solution))\n",
    "plt.xlabel('Exact Solution')\n",
    "plt.ylabel('DPDE Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efca54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples, t_samples, s1_samples, s2_samples, riskfree_rate_samples, \\\n",
    "  volatility1_samples, volatility2_samples, correlation_samples = \\\n",
    "              get_random_points_of_interest(nr_samples_error_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predict {} values and measure the time:'.format(nr_samples_error_calculation))\n",
    "%time DPDE_solution = model.predict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df97290",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_solution_evaluated = [exact_solution(t=t, s1=s1, s2=s2, \n",
    "                                  riskfree_rate=riskfree_rate,\n",
    "                                  volatility1=volatility1,\n",
    "                                  volatility2=volatility2,\n",
    "                                  correlation=correlation) \n",
    "            for t, s1, s2, riskfree_rate, volatility1, volatility2, correlation \n",
    "            in zip(t_samples, s1_samples, s2_samples, riskfree_rate_samples, \n",
    "                    volatility1_samples, volatility2_samples,\n",
    "                    correlation_samples)]\n",
    "\n",
    "exact_solution_evaluated = np.array(exact_solution_evaluated).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1799cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated MSE Error:')\n",
    "print(np.sqrt(np.mean(np.square(exact_solution_evaluated - DPDE_solution))))\n",
    "\n",
    "print('Relative Error to L2 Norm in %:')\n",
    "print(np.sqrt(np.mean(np.square(exact_solution_evaluated - DPDE_solution))) \n",
    "        / np.sqrt(np.mean(np.square(exact_solution_evaluated)))*100)\n",
    "\n",
    "print('Maximal Error:')\n",
    "print(np.max(exact_solution_evaluated - DPDE_solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c01a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated MSE Error:')\n",
    "print(0.030773506093868433)\n",
    "print('Relative Error to L2 Norm in %:')\n",
    "print(0.08530248578852807)\n",
    "print('Maximal Error:')\n",
    "print(0.1987940127602612)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e120194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1af0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
